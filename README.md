# Agentic RAG Chatbot

> Fully local · No API keys · Multi-user · Real-time streaming

---

## Participant Info

| Field | Value |
|-------|-------|
| Full Name | [NAME] |
| Email | [EMAIL] |
| GitHub Username | [GITHUB] |

---

## Video Walkthrough

[PASTE LINK HERE]

---

## Features Implemented

| Feature | Status |
|---------|--------|
| A — File Upload + RAG with Citations | ✅ |
| A+ — Hybrid Retrieval (BM25 + FAISS + HyDE + CrossEncoder Reranking) | ✅ |
| A+ — Smart Chunking (Section-aware + Semantic boundaries) | ✅ |
| A+ — Knowledge Graph RAG | ✅ |
| B — Persistent Memory (USER_MEMORY + COMPANY_MEMORY) | ✅ |
| C — Open-Meteo Sandbox Agent | ✅ |
| Multi-User Isolation (per-user FAISS + memory + graph) | ✅ |
| Real-Time SSE Streaming | ✅ |
| Redis Query + Embedding Cache | ✅ |

---

## Quick Start

### Prerequisites
- Python 3.11+
- [Ollama](https://ollama.com) installed and running
- Redis (optional — app works without it)

### 1. Install Ollama and pull models (one-time, ~9GB)
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1:8b
ollama pull mistral:7b
ollama pull nomic-embed-text
```

### 2. Clone and set up
```bash
git clone <YOUR_REPO_URL>
cd agentic-rag-chatbot-template
make dev-setup
```

### 3. Configure environment
```bash
cp .env.example .env
# Defaults work out of the box.
# Change JWT_SECRET before any production use.
```

### 4. (Optional) Start Redis
```bash
docker run -d -p 6379:6379 redis:7-alpine
```

### 5. Run
```bash
make run
# Open http://localhost:8000
```

### 6. Sanity check (judge command)
```bash
make sanity
# → artifacts/sanity_output.json
```

### 7. CI / no Ollama
```bash
SANITY_MOCK=true make sanity
```

---

## Demo Flow

### Feature A — Upload + RAG + Citations
1. Register an account at http://localhost:8000
2. Drag and drop a PDF onto the upload zone
3. Watch live progress: loading → chunking → embedding → indexing
4. Ask a question — answer streams token by token with inline citations
5. Expand citation cards: source filename, section, page, exact chunk text
6. Try: "compare the methods in both papers" — cross-doc synthesis activates

### Feature B — Memory
1. Say "I'm a machine learning researcher" in chat
2. Check the Memory tab after a few seconds — fact appears in USER_MEMORY.md
3. Mention "our team always bottlenecks on document review" → check COMPANY_MEMORY.md
4. Memory is injected into RAG context for personalized answers

### Feature C — Open-Meteo Sandbox
1. Ask: "Analyse temperature trends in London over the last 30 days"
2. Agent calls Open-Meteo, fetches time series, computes rolling averages and anomaly flags
3. Returns structured findings

---

## Architecture

See [ARCHITECTURE.md](ARCHITECTURE.md)

---

## Project Structure

```
app/
  main.py            FastAPI app, lifespan, all routes
  config.py          Pydantic Settings from .env
  db.py              aiosqlite schema and helpers
  schemas.py         Pydantic models
  core/
    ingest.py        Section-aware chunking, async embedding, background ingest
    retriever.py     HyDE + Ensemble + Reranking + Parent expansion + Graph context
    memory.py        Fact extraction, memory file writes
    graph.py         Knowledge graph build and query
    cache.py         Redis embedding and query cache
    sandbox.py       Open-Meteo REACT agent
  auth/
    middleware.py    JWT validation
    router.py        /users/register and /users/login
  templates/
    index.html       Three-panel streaming workspace UI
static/style.css
static/app.js
frontend/             React + Tailwind (optional)
vectorstore/user_{id}/   Per-user FAISS shard
memory/user_{id}/        USER_MEMORY.md + COMPANY_MEMORY.md
data/ragbot.db           SQLite
artifacts/               Generated by make sanity
scripts/init_db.py
scripts/sanity_check.py
scripts/sanity_check.sh
tests/
```

---

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| OLLAMA_BASE_URL | http://localhost:11434 | Ollama server |
| OLLAMA_CHAT_MODEL | llama3.1:8b | Chat and RAG model |
| OLLAMA_JSON_MODEL | mistral:7b | Memory extraction |
| OLLAMA_EMBED_MODEL | nomic-embed-text | Embeddings |
| REDIS_URL | (empty) | Leave blank to disable cache |
| DB_PATH | data/ragbot.db | SQLite path |
| JWT_SECRET | change-me | JWT signing key |
| HYDE_ENABLED | true | HyDE retrieval expansion |
| MEMORY_CONFIDENCE_THRESHOLD | 0.80 | Min confidence to write memory |
| SANITY_MOCK | false | Skip Ollama in CI |

---

## Makefile Reference

| Command | Description |
|---------|-------------|
| make dev-setup | Full first-time setup |
| make install | Install Python deps only |
| make pull-models | Pull all Ollama models |
| make run | Dev server with hot reload |
| make run-fast | 4 workers, production-like |
| make sanity | End-to-end sanity check |
| make test | Run pytest suite |
| make lint | ruff + mypy |
| make clean | Remove all generated files |

---

## Key Design Decisions and Tradeoffs

| Decision | Tradeoff |
|----------|----------|
| Hierarchical parent-child chunking | ~2× more SQLite storage — significantly better answer quality |
| HyDE retrieval | One extra Ollama round-trip (~2s) — better precision on technical docs |
| CrossEncoder reranking | Requires sentence-transformers (~400MB), loaded once at startup |
| Ollama over OpenAI | Zero cost, fully local — slower on CPU, weaker than GPT-4o |
| Per-user FAISS shards | Clean isolation — no cross-user document sharing |
| aiosqlite over PostgreSQL | Zero infra, async-native — needs migration at production scale |
| Redis optional | Simpler setup for judges — repeat queries uncached without it |

---

## What I Would Improve Next

- Fine-tune the cross-encoder on domain-specific data for better citation quality
- Memory consolidation: weekly LLM job to compress older entries
- Replace in-memory BM25 with persisted index (Elasticsearch or Typesense)
- Document versioning: re-index updated files without full re-upload
- Docker Compose file bundling Redis + Ollama + app for one-command deployment
